{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2lzL6ia3EKW",
        "outputId": "7c87dab6-0f76-4626-992f-1d4ff651ec56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Nov 25 03:10:28 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCVqOuDF3JHr",
        "outputId": "2b2b0c63-75bf-41fb-db8d-6c2326324fae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving folder list\n",
            "Processing file 18uq-Lscu7vjF_IriXXTDS3vIdUCpOC_Y private_test.json\n",
            "Processing file 1Oxlwz919OLUkLjKlUfJB5pZ49wsGhxG4 public_test.json\n",
            "Processing file 1ICiCPKxVIV3TCOPrhLIrVurDWD3uME9P train.json\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18uq-Lscu7vjF_IriXXTDS3vIdUCpOC_Y\n",
            "To: /content/data/private_test.json\n",
            "100% 48.4k/48.4k [00:00<00:00, 122MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Oxlwz919OLUkLjKlUfJB5pZ49wsGhxG4\n",
            "To: /content/data/public_test.json\n",
            "100% 74.3k/74.3k [00:00<00:00, 119MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ICiCPKxVIV3TCOPrhLIrVurDWD3uME9P\n",
            "To: /content/data/train.json\n",
            "100% 2.94M/2.94M [00:00<00:00, 187MB/s]\n",
            "Download completed\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.24.1\n",
            "Collecting transformers==4.34.1\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.1)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.1)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.1) (2023.7.22)\n",
            "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.19.4\n",
            "    Uninstalling huggingface-hub-0.19.4:\n",
            "      Successfully uninstalled huggingface-hub-0.19.4\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed huggingface-hub-0.17.3 tokenizers-0.14.1 transformers-4.34.1\n",
            "Collecting bitsandbytes==0.41.1\n",
            "  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.41.1\n",
            "Collecting peft==0.6.0\n",
            "  Downloading peft-0.6.0-py3-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0) (4.34.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0) (4.66.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0) (0.24.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0) (0.4.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->peft==0.6.0) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.0) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.6.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.6.0) (0.14.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.6.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.6.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.6.0) (1.3.0)\n",
            "Installing collected packages: peft\n",
            "Successfully installed peft-0.6.0\n",
            "Collecting datasets==2.5.2\n",
            "  Downloading datasets-2.5.2-py3-none-any.whl (432 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.7/432.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.5.2) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.5.2) (9.0.0)\n",
            "Collecting dill<0.3.6 (from datasets==2.5.2)\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.5.2) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.5.2) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.5.2) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.5.2) (3.4.1)\n",
            "Collecting multiprocess (from datasets==2.5.2)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.5.2) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.5.2) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.5.2) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.5.2) (23.2)\n",
            "Collecting responses<0.19 (from datasets==2.5.2)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.5.2) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.5.2) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.5.2) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.5.2) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.5.2) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.5.2) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.5.2) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.5.2) (3.13.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.5.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.5.2) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.5.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.5.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.5.2) (2023.7.22)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.5.2)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading multiprocess-0.70.13-py310-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.5.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.5.2) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.5.2) (1.16.0)\n",
            "Installing collected packages: dill, responses, multiprocess, datasets\n",
            "Successfully installed datasets-2.5.2 dill-0.3.5.1 multiprocess-0.70.13 responses-0.18.0\n",
            "Collecting evaluate==0.4.0\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0) (2.5.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0) (0.3.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0) (0.70.13)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0) (23.2)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate==0.4.0) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate==0.4.0) (3.8.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.4.0) (3.13.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.4.0) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.4.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate==0.4.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate==0.4.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate==0.4.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate==0.4.0) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate==0.4.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate==0.4.0) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate==0.4.0) (1.16.0)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.0\n",
            "Collecting sentencepiece==0.1.99\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "! gdown https://drive.google.com/drive/folders/1klScaSneR_mOOYrOTF3T1ppeWACr6sbD -O /content/data --folder\n",
        "! pip install accelerate\n",
        "! pip install transformers==4.34.1\n",
        "! pip install bitsandbytes==0.41.1\n",
        "! pip install peft==0.6.0\n",
        "! pip install datasets==2.5.2\n",
        "! pip install evaluate==0.4.0\n",
        "! pip install sentencepiece==0.1.99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSrswyMA3L4U",
        "outputId": "42ca0b0f-bdd8-4829-de8c-10763af62653"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuda Is Available\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import copy\n",
        "import json\n",
        "import os\n",
        "from os.path import exists, join, isdir\n",
        "from dataclasses import dataclass, field\n",
        "import sys\n",
        "from typing import Optional, Dict, Sequence\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import bitsandbytes as bnb\n",
        "import pandas as pd\n",
        "import importlib\n",
        "from packaging import version\n",
        "from packaging.version import parse\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    print(\"Cuda Is Available\")\n",
        "\n",
        "import transformers\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import argparse\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    set_seed,\n",
        "    Seq2SeqTrainer,\n",
        "    BitsAndBytesConfig,\n",
        "    LlamaTokenizer\n",
        "\n",
        ")\n",
        "from datasets import load_dataset, Dataset\n",
        "import evaluate\n",
        "\n",
        "from peft import (\n",
        "    prepare_model_for_kbit_training,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    PeftModel\n",
        ")\n",
        "from peft.tuners.lora import LoraLayer\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "\n",
        "from utils import get_prompt, get_bnb_config\n",
        "from ppl import perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6oIx8wW83Umz"
      },
      "outputs": [],
      "source": [
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "\n",
        "def model_parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Finetune a Llama Model With Adaptor by Instruction-Tuning\")\n",
        "\n",
        "    # model arguments\n",
        "    parser.add_argument(\"--model_name_or_path\", type=str, default=\"./model/Taiwan-LLM-7B-v2.0-chat\")\n",
        "    parser.add_argument(\"--trust_remote_code\", type=bool, default=True)\n",
        "\n",
        "    args = parser.parse_known_args()[0]\n",
        "    return args\n",
        "\n",
        "def data_parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Finetune a Llama Model With Adaptor by Instruction-Tuning\")\n",
        "\n",
        "    # data arguments\n",
        "    parser.add_argument(\"--train_file\", type=str, default=\"./data/train.json\")\n",
        "    parser.add_argument(\"--validation_file\", type=str, default=\"./data/public_test.json\")\n",
        "    parser.add_argument(\"--test_file\", type=str, default=\"./data/private_test.json\")\n",
        "    parser.add_argument(\"--result_file\", type=str, default=\"./prediction.json\")\n",
        "    parser.add_argument(\"--max_source_len\", type=int, default=256)\n",
        "    parser.add_argument(\"--max_target_len\", type=int, default=128)\n",
        "\n",
        "    args = parser.parse_known_args()[0]\n",
        "    if args.train_file is None or args.validation_file is None or args.result_file is None:\n",
        "        raise ValueError(\"Need train file, validation file, and the result file specification\")\n",
        "    else:\n",
        "        # neither do train_file nor validation file is None, so\n",
        "        extension = args.train_file.split(\".\")[-1] # to see what extension the file is\n",
        "        print(extension)\n",
        "        assert extension == \"json\", \"train_file should be a json file\"\n",
        "        extension = args.validation_file.split(\".\")[-1] # to see what extension the file is\n",
        "        assert extension == \"json\", \"validation_file should be a json file\"\n",
        "        extension = args.test_file.split(\".\")[-1] # to see what extension the file is\n",
        "        assert extension == \"json\", \"test_file should be a json file\"\n",
        "        extension = args.result_file.split(\".\")[-1] # to see what extension the file is\n",
        "        assert extension == \"json\", \"result_file should be a json file\"\n",
        "    return args\n",
        "\n",
        "class TrainingArguments(transformers.Seq2SeqTrainingArguments):\n",
        "    cache_dir: str = field(\n",
        "        default=\"./model/\"\n",
        "    )\n",
        "    double_quant: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Compress the quantization statistics through double quantization.\"}\n",
        "    )\n",
        "    quant_type: str = field(\n",
        "        default=\"fp4\",\n",
        "        metadata={\"help\": \"Quantization data type to use. Should be one of `fp4` or `nf4`.\"}\n",
        "    )\n",
        "    lora_r: int = field(\n",
        "        default=64,\n",
        "        metadata={\"help\": \"Lora R dimension.\"}\n",
        "    )\n",
        "    lora_alpha: float = field(\n",
        "        default=16,\n",
        "        metadata={\"help\": \" Lora alpha.\"}\n",
        "    )\n",
        "    lora_dropout: float = field(\n",
        "        default=0.0,\n",
        "        metadata={\"help\":\"Lora dropout.\"}\n",
        "    )\n",
        "    def __init__(self, cache_dir:str, double_quant:bool, quant_type:str, lora_r:int, lora_alpha:float, lora_dropout:float, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.cache_dir = cache_dir\n",
        "        self.double_quant = double_quant\n",
        "        self.quant_type = quant_type\n",
        "        self.lora_r = lora_r\n",
        "        self.lora_alpha = lora_alpha\n",
        "        self.lora_dropout = lora_dropout\n",
        "\n",
        "def train_parse_args():\n",
        "    args = TrainingArguments(\n",
        "            cache_dir=\"./model/\",\n",
        "            double_quant=True,\n",
        "            quant_type=\"fp4\",\n",
        "            lora_r=64,\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0.0,\n",
        "\n",
        "            output_dir=\"./model/\",\n",
        "            optim=\"paged_adamw_32bit\",\n",
        "            per_device_train_batch_size=1,\n",
        "            gradient_accumulation_steps=4,\n",
        "            max_steps=250,\n",
        "            weight_decay=0.0,\n",
        "            learning_rate=1e-4,\n",
        "            remove_unused_columns=False,\n",
        "            max_grad_norm=0.3,\n",
        "            gradient_checkpointing=True,\n",
        "            do_train=True,\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            warmup_ratio=0.03,\n",
        "            logging_steps=10, # FIXME10\n",
        "            group_by_length=True,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=25, #FIXME 250\n",
        "            save_total_limit=50,\n",
        "    )\n",
        "    return args\n",
        "\n",
        "def gen_parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Finetune a Llama Model With Adaptor by Instruction-Tuning\")\n",
        "\n",
        "    # generation arguments\n",
        "    parser.add_argument(\"--max_new_tokens\", type=int, default=64)\n",
        "\n",
        "    # Generation strategy\n",
        "    parser.add_argument(\"--do_sample\", type=bool, default=False)\n",
        "    parser.add_argument(\"--num_beams\", type=int, default=3)\n",
        "    parser.add_argument(\"--num_beam_groups\", type=int, default=1)\n",
        "    parser.add_argument(\"--use_cache\", type=bool, default=True)\n",
        "\n",
        "    # Hyperparameters for logit manipulation\n",
        "    parser.add_argument(\"--temperature\", type=float, default=1.0)\n",
        "    parser.add_argument(\"--top_k\", type=int, default=50)\n",
        "    parser.add_argument(\"--top_p\", type=float, default=1.0)\n",
        "    parser.add_argument(\"--typical_p\", type=float, default=1.0)\n",
        "    parser.add_argument(\"--diversity_penalty\", type=float, default=0.0)\n",
        "    parser.add_argument(\"--repetition_penalty\", type=float, default=1.0)\n",
        "    parser.add_argument(\"--length_penalty\", type=float, default=1.0)\n",
        "    parser.add_argument(\"--no_repeat_ngram_size\", type=int, default=0)\n",
        "\n",
        "    args = parser.parse_known_args()[0]\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KiYi_hxXAscl"
      },
      "outputs": [],
      "source": [
        "class SavePeftModelCallback(transformers.TrainerCallback):\n",
        "    def save_model(self, args, state, kwargs):\n",
        "        print('Saving PEFT checkpoint...')\n",
        "        kwargs[\"model\"].save_pretrained(args.output_dir)\n",
        "\n",
        "        pytorch_model_path = os.path.join(args.output_dir, \"pytorch_model.bin\")\n",
        "        if os.path.exists(pytorch_model_path):\n",
        "            os.remove(pytorch_model_path)\n",
        "\n",
        "    def on_save(self, args, state, control, **kwargs):\n",
        "        self.save_model(args, state, kwargs)\n",
        "        return control\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        def touch(fname, times=None):\n",
        "            with open(fname, 'a'):\n",
        "                os.utime(fname, times)\n",
        "\n",
        "        touch(join(args.output_dir, 'completed'))\n",
        "        self.save_model(args, state, kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7jI3-GwzBUQl"
      },
      "outputs": [],
      "source": [
        "def smart_tokenizer_and_embedding_resize(\n",
        "    special_tokens_dict: Dict,\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        "    model: transformers.PreTrainedModel,\n",
        "):\n",
        "    \"\"\"Resize tokenizer and embedding.\n",
        "\n",
        "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
        "    \"\"\"\n",
        "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if num_new_tokens > 0:\n",
        "        input_embeddings_data = model.get_input_embeddings().weight.data\n",
        "        output_embeddings_data = model.get_output_embeddings().weight.data\n",
        "\n",
        "        input_embeddings_avg = input_embeddings_data[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "        output_embeddings_avg = output_embeddings_data[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "\n",
        "        input_embeddings_data[-num_new_tokens:] = input_embeddings_avg\n",
        "        output_embeddings_data[-num_new_tokens:] = output_embeddings_avg\n",
        "\n",
        "def get_accelerate_model(args, checkpoint_dir=None):\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        cache_dir=args.cache_dir,\n",
        "        load_in_4bit=True,\n",
        "        load_in_8bit=False,\n",
        "        quantization_config=get_bnb_config(),\n",
        "        torch_dtype=torch.float32,\n",
        "        trust_remote_code=args.trust_remote_code,\n",
        "        use_auth_token=False,\n",
        "    )\n",
        "\n",
        "    setattr(model, 'model_parallel', True)\n",
        "    setattr(model, 'is_parallelizable', True)\n",
        "\n",
        "    model.config.torch_dtype = torch.float32\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        cache_dir=args.cache_dir,\n",
        "        padding_side=\"right\",\n",
        "        use_fast=False,\n",
        "        tokenizer_type=\"llama\",\n",
        "        trust_remote_code=args.trust_remote_code,\n",
        "        use_auth_token=False,\n",
        "    )\n",
        "\n",
        "    if tokenizer._pad_token is None:\n",
        "        smart_tokenizer_and_embedding_resize(\n",
        "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
        "            tokenizer=tokenizer,\n",
        "            model=model,\n",
        "        )\n",
        "\n",
        "\n",
        "    print('Adding special tokens.')\n",
        "    tokenizer.add_special_tokens({\n",
        "            \"eos_token\": tokenizer.convert_ids_to_tokens(model.config.eos_token_id),\n",
        "            \"bos_token\": tokenizer.convert_ids_to_tokens(model.config.bos_token_id),\n",
        "            \"unk_token\": tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id),\n",
        "    })\n",
        "\n",
        "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.gradient_checkpointing)\n",
        "\n",
        "    # modules = find_all_linear_names(args, model)\n",
        "    # print(modules)\n",
        "    if checkpoint_dir is not None:\n",
        "        print(\"Loading adapters from checkpoint.\")\n",
        "        model = PeftModel.from_pretrained(model, checkpoint_dir, is_trainable=True)\n",
        "    else:\n",
        "        print(f'adding LoRA modules...')\n",
        "        config = LoraConfig(\n",
        "            r=args.lora_r,\n",
        "            lora_alpha=args.lora_alpha,\n",
        "            lora_dropout=args.lora_dropout,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "        )\n",
        "        model = get_peft_model(model, config)\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if 'norm' in name:\n",
        "            module = module.to(torch.float32)\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Upq1Ry0uFmVh"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollatorForCausalLM(object):\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "    source_max_len: int\n",
        "    target_max_len: int\n",
        "    train_on_source: bool\n",
        "    predict_with_generate: bool\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        # Extract elements\n",
        "        sources = [f\"{self.tokenizer.bos_token}{example['input']}\" for example in instances]\n",
        "        targets = [f\"{example['output']}{self.tokenizer.eos_token}\" if \"output\" in example.keys() else \"\" for example in instances]\n",
        "\n",
        "        # Tokenize\n",
        "        tokenized_sources_with_prompt = self.tokenizer(\n",
        "            sources,\n",
        "            max_length=self.source_max_len,\n",
        "            truncation=True,\n",
        "            add_special_tokens=False,\n",
        "        )\n",
        "        tokenized_targets = self.tokenizer(\n",
        "            targets,\n",
        "            max_length=self.target_max_len,\n",
        "            truncation=True,\n",
        "            add_special_tokens=False,\n",
        "        )\n",
        "        # Build the input and labels for causal LM\n",
        "        input_ids = []\n",
        "        labels = []\n",
        "        for tokenized_source, tokenized_target in zip(\n",
        "            tokenized_sources_with_prompt['input_ids'],\n",
        "            tokenized_targets['input_ids']\n",
        "        ):\n",
        "            if not self.predict_with_generate:\n",
        "                input_ids.append(torch.tensor(tokenized_source + tokenized_target))\n",
        "                if not self.train_on_source:\n",
        "                    labels.append(\n",
        "                        torch.tensor([IGNORE_INDEX for _ in range(len(tokenized_source))] + copy.deepcopy(tokenized_target))\n",
        "                    )\n",
        "                else:\n",
        "                    labels.append(torch.tensor(copy.deepcopy(tokenized_source + tokenized_target)))\n",
        "            else:\n",
        "                input_ids.append(torch.tensor(tokenized_source))\n",
        "\n",
        "        # Apply padding\n",
        "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        labels = pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX) if not self.predict_with_generate else None\n",
        "        data_dict = {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask':input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        }\n",
        "        if labels is not None:\n",
        "            data_dict['labels'] = labels\n",
        "        return data_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "larjVbU-HPC4"
      },
      "outputs": [],
      "source": [
        "def load_dataset(tokenizer: transformers.PreTrainedTokenizer, args) -> Dict:\n",
        "    # dataset = dataset.map()\n",
        "    train_dataset = Dataset.from_json(path_or_paths=args.train_file)\n",
        "    eval_dataset = Dataset.from_json(path_or_paths=args.validation_file)\n",
        "    test_dataset = Dataset.from_json(path_or_paths=args.test_file)\n",
        "\n",
        "    train_dataset = train_dataset.map(lambda x: {\n",
        "        'input': get_prompt(x['instruction']),\n",
        "        'output': x['output']\n",
        "    })\n",
        "\n",
        "    if args.group_by_length:\n",
        "        train_dataset = train_dataset.map(lambda x: {'length': len(x['input']) + len(x['output'])})\n",
        "\n",
        "    data_collator = DataCollatorForCausalLM(\n",
        "        tokenizer=tokenizer,\n",
        "        source_max_len=args.max_source_len,\n",
        "        target_max_len=args.max_target_len,\n",
        "        predict_with_generate=False,\n",
        "        train_on_source=False,\n",
        "    )\n",
        "\n",
        "    return dict(\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        test_dataset=test_dataset,\n",
        "        data_collator=data_collator\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3HR4sH-KJJg"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBB1NgTiXvTf",
        "outputId": "dcb6917a-4596-49f5-dfdb-d0dff91fe3df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "json\n",
            "Namespace(model_name_or_path='/content/drive/MyDrive/ADL/Hw3/Taiwan-LLM-7B-v2.0-chat', trust_remote_code=True, train_file='/content/data/train.json', validation_file='/content/data/public_test.json', test_file='/content/data/private_test.json', result_file='/content/prediction.json', max_source_len=256, max_target_len=128, output_dir='/content/model/', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=4, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=250, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/content/model/runs/Nov26_09-32-26_113d16ea3acc', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=25, save_total_limit=50, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, past_index=-1, run_name='/content/model/', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, include_tokens_per_second=False, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {\n",
            "  \"max_new_tokens\": 64,\n",
            "  \"num_beams\": 3\n",
            "}\n",
            ", distributed_state=Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            ", _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None, cache_dir='/content/model/', double_quant=True, quant_type='fp4', lora_r=64, lora_alpha=16, lora_dropout=0.0, max_new_tokens=64, do_sample=False, num_beams=3, num_beam_groups=1, use_cache=True, temperature=1.0, top_k=50, top_p=1.0, typical_p=1.0, diversity_penalty=0.0, repetition_penalty=1.0, length_penalty=1.0, no_repeat_ngram_size=0)\n"
          ]
        }
      ],
      "source": [
        "model_args = model_parse_args()\n",
        "data_args = data_parse_args()\n",
        "train_args = train_parse_args()\n",
        "gen_args = gen_parse_args()\n",
        "train_args.generation_config = transformers.GenerationConfig(**vars(gen_args))\n",
        "args = argparse.Namespace(**vars(model_args), **vars(data_args), **vars(train_args), **vars(gen_args))\n",
        "print(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226,
          "referenced_widgets": [
            "eb77f7e7198848c9bec36321f093f282",
            "be3f6d2f3a7f44109df8ff3f9d2b5b3b",
            "7a3f66bf9bf04efea1ecdd99b029da2c",
            "a167cc80bdad4c9dbb629e7cd053f203",
            "43fdac7e6e814fb9bb70d3a83e14d70e",
            "4db492a34dd340febaab49847e0a5011",
            "37fe5ca34ce94eadaa13307613bef13a",
            "5a74a0636613462e86a7a4ce27af594a",
            "cda2b0e3d3534f22a71d02bc0485187a",
            "7bdeee7048a44c89a7887714350b6eef",
            "1c93c7cf6a2b479d9b9f9e9c6bca24ee"
          ]
        },
        "id": "PPSodMtgKGwF",
        "outputId": "98632641-1037-478a-f139-a0474e59153c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb77f7e7198848c9bec36321f093f282",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:655: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding special tokens.\n",
            "Loading adapters from checkpoint.\n",
            "loaded model\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = get_accelerate_model(args)\n",
        "\n",
        "model.config.use_cache = False\n",
        "print('loaded model')\n",
        "set_seed(args.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74sjezGHV27g",
        "outputId": "24193955-9b5b-4b74-d576-14ba8e59e931"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-1838e29a23ef5553\n",
            "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1838e29a23ef5553/0.0.0)\n",
            "WARNING:datasets.builder:Using custom data configuration default-18d3519825b860b9\n",
            "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/json/default-18d3519825b860b9/0.0.0)\n",
            "WARNING:datasets.builder:Using custom data configuration default-bcb5b2a61015c533\n",
            "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/json/default-bcb5b2a61015c533/0.0.0)\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1838e29a23ef5553/0.0.0/cache-d53323a238384154.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1838e29a23ef5553/0.0.0/cache-bcc2f9bb9832bd6b.arrow\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 33,554,432 || all params: 6,771,970,048 || trainable%: 0.49548996469513035\n",
            "torch.float32 295964672 0.043704368138398474\n",
            "torch.int8 6476005376 0.9562956318616015\n"
          ]
        }
      ],
      "source": [
        "data_module = load_dataset(tokenizer=tokenizer, args=args)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=train_args,\n",
        "    **{k:v for k,v in data_module.items() if k != 'test_dataset'},\n",
        ")\n",
        "\n",
        "## call_backs\n",
        "trainer.add_callback(SavePeftModelCallback)\n",
        "\n",
        "# Verifying the datatypes and parameter counts before training.\n",
        "model.print_trainable_parameters()\n",
        "dtypes = {}\n",
        "for _, p in model.named_parameters():\n",
        "    dtype = p.dtype\n",
        "    if dtype not in dtypes: dtypes[dtype] = 0\n",
        "    dtypes[dtype] += p.numel()\n",
        "total = 0\n",
        "for k, v in dtypes.items(): total+= v\n",
        "for k, v in dtypes.items():\n",
        "    print(k, v, v/total)\n",
        "\n",
        "# All metric\n",
        "all_metrics = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OhpEKEnIMOUA",
        "outputId": "7dcb6f2f-a238-45f4-d497-95961a4a9075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** Train ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='57' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 57/250 07:58 < 28:00, 0.11 it/s, Epoch 0.02/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.565700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.444600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.168300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.209500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.194400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving PEFT checkpoint...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving PEFT checkpoint...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 35:21, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.565700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.444600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.168300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.209500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.194400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.540200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.325500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.373300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.206600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.091100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.389700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.156700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.094700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.307000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.072200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.262100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.264200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.232900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.258100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.289100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.611700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.245500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.065900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.310200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.081100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving PEFT checkpoint...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving PEFT checkpoint...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving PEFT checkpoint...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving PEFT checkpoint...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving PEFT checkpoint...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving PEFT checkpoint...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving PEFT checkpoint...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving PEFT checkpoint...\n",
            "Saving PEFT checkpoint...\n",
            "***** train metrics *****\n",
            "  epoch                    =        0.1\n",
            "  total_flos               =  7457146GF\n",
            "  train_loss               =     1.2704\n",
            "  train_runtime            = 0:35:33.21\n",
            "  train_samples_per_second =      0.469\n",
            "  train_steps_per_second   =      0.117\n"
          ]
        }
      ],
      "source": [
        "print(\"*** Train ***\")\n",
        "train_result = trainer.train()\n",
        "metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()\n",
        "\n",
        "all_metrics.update(metrics)\n",
        "loss_plt_list = [example[\"loss\"] for example in trainer.state.log_history[:-1]] # the last one has no \"loss\" key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca15Ibm3MPJh",
        "outputId": "b223d97e-431a-446a-90d7-a40fb7b5a5d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** Evaluate ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/250 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "100%|██████████| 250/250 [01:32<00:00,  2.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** eval metrics *****\n",
            "  num_example\t\t=\t250\n",
            "  mean_perplexity\t=\t3.9474233360290527\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"*** Evaluate ***\")\n",
        "eval_result = perplexity(model=model, tokenizer=tokenizer, data=data_module[\"eval_dataset\"], max_length=2048)\n",
        "print(\"***** eval metrics *****\")\n",
        "print(\"  num_example\\t\\t=\\t{}\".format(len(data_module[\"eval_dataset\"])))\n",
        "print(\"  mean_perplexity\\t=\\t{}\".format(eval_result[\"mean_perplexity\"]))\n",
        "all_metrics.update({\"num_example\":len(data_module[\"eval_dataset\"]), \"mean_perplexity\":eval_result[\"mean_perplexity\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J1EFgUiMipM",
        "outputId": "6ec86dbf-23db-419b-ca66-fcd8c7dfc748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** Test ***\n",
            "  num_example = 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/250 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1539: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "100%|██████████| 250/250 [1:05:18<00:00, 15.67s/it]\n",
            "100%|██████████| 250/250 [1:03:27<00:00, 15.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** Prediction Samples *****\n",
            "[{'id': 'd573ddd1-7bb9-468d-b906-e392223d9579', 'output': '你是人工智慧助理，以下是用戶和人工智能助理之間的對話。你要對用戶的問題提供有用、安全、詳細和禮貌的回答。USER: 穿右穴而進，其下甚削，陷峽頗深，即下穿所入之峽也，以為右穴。\\n\\n這是哪種語言？\\nASSISTANT: 中文 你是人工智慧助理，以下是用戶和人工智能助理之間的�'}, {'id': 'e3c475ca-f2b2-4450-af6d-675e646c2488', 'output': '你是人工智慧助理，以下是用戶和人工智能助理之間的對話。你要對用戶的問題提供有用、安全、詳細和禮貌的回答。USER: 東南水旱，盜賊常常發生，西、北二國窺伺日久，怎麼能不預先準備呢？\\n\\n答：東南水旱，盜賊常常發生，西、北二國窺伺日久，怎麼能不預先��'}, {'id': 'ba1bdb44-0baa-447d-b041-4169716a7d5b', 'output': '你是人工智慧助理，以下是用戶和人工智能助理之間的對話。你要對用戶的問題提供有用、安全、詳細和禮貌的回答。USER: 闥活捉一豬，從頭咬至頂，放之地上，仍走。\\n把這個豬活捉起來，從頭咬至頂，放在地上，它仍然走著。ASSISTANT: 是 你是人工智��'}, {'id': '66189e2e-b1aa-475f-a9cf-1b1cbebc80b1', 'output': '你是人工智慧助理，以下是用戶和人工智能助理之間的對話。你要對用戶的問題提供有用、安全、詳細和禮貌的回答。USER: 翻譯成文言文：\\n現在學者大儒，都各自年事已高，教導訓誡的方法，與當時不同。\\n答案:ASSISTANT: 當時學者大儒，各自年事已高，教導訓誡的方法，與當時不'}, {'id': 'efd742ec-8f0d-46f1-afff-6c34d468eeb0', 'output': '你是人工智慧助理，以下是用戶和人工智能助理之間的對話。你要對用戶的問題提供有用、安全、詳細和禮貌的回答。USER: 文言文翻譯：\\n成王既幼，周公攝政，當國踐祚，召公疑之，曰：「吾幼，周公攝政，當國踐祚，召公疑之，曰：「吾幼，周公攝政，當國�'}]\n"
          ]
        }
      ],
      "source": [
        "# Using the trainer prediction function would cause CUDA out of memory\n",
        "print(\"*** Test ***\")\n",
        "print(\"  num_example = {}\".format(len(data_module[\"test_dataset\"])))\n",
        "\n",
        "model.eval()\n",
        "test_dataset = data_module[\"test_dataset\"]\n",
        "test_result_lst = []\n",
        "progress_bar = tqdm(range(len(test_dataset)), position=0, leave=True)\n",
        "for idx, example in enumerate(test_dataset):\n",
        "    input_ids = torch.tensor([tokenizer.bos_token_id] + \\\n",
        "                    tokenizer.encode(text=get_prompt(example['instruction']), add_special_tokens=False, max_length=args.max_target_len)).unsqueeze(0)\n",
        "    prediction_ids = model.generate(input_ids=input_ids, generation_config=train_args.generation_config)\n",
        "    prediction = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "    test_result_lst.append({\"id\":example[\"id\"], \"output\": prediction})\n",
        "    progress_bar.update(1)\n",
        "\n",
        "# Verifying the result\n",
        "print(\"***** Prediction Samples *****\")\n",
        "print(test_result_lst[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBk45GtRgssU",
        "outputId": "fa2b0f03-2b26-4e8f-fe1d-6bc6d0ac1d8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'd573ddd1-7bb9-468d-b906-e392223d9579', 'output': '中文 '}\n",
            "{'id': 'e3c475ca-f2b2-4450-af6d-675e646c2488', 'output': '東南水旱，盜賊常常發生，西、北二國窺伺日久，怎麼能不預先��'}\n",
            "{'id': 'ba1bdb44-0baa-447d-b041-4169716a7d5b', 'output': '是 '}\n",
            "{'id': '66189e2e-b1aa-475f-a9cf-1b1cbebc80b1', 'output': '當時學者大儒，各自年事已高，教導訓誡的方法，與當時不'}\n",
            "{'id': 'efd742ec-8f0d-46f1-afff-6c34d468eeb0', 'output': '「吾幼，周公攝政，當國�'}\n",
            "{'id': 'e98b8f5a-6d09-4cfe-96be-0562a26383ac', 'output': ''}\n",
            "{'id': '7efea98b-646a-4bd8-b85c-0118d3493506', 'output': ' '}\n",
            "{'id': '3c631802-79cf-4b2e-bf8a-8ced4ac60a84', 'output': '說災異是為瞭譴告和懲罰'}\n",
            "{'id': 'f525edb2-3118-4f95-827e-b644572f41f0', 'output': '�'}\n",
            "{'id': 'a5c7bb63-07f7-4e10-b04c-89b436d841da', 'output': ' '}\n",
            "{'id': '92636fb8-dca3-4ca0-9855-fb05e4724662', 'output': ' '}\n",
            "{'id': '4b391e6d-fce0-4c71-a75d-e5bc2e3dfb4a', 'output': ' '}\n",
            "{'id': '0f00462c-ae71-4410-81d6-f07d64b61d33', 'output': '如果兩大勢都傲慢，則必互相滅'}\n",
            "{'id': '9cc474be-04f9-46b1-9fbf-bba5f35e074c', 'output': ' '}\n",
            "{'id': '33178d9c-fd28-4324-8ac7-cc4ad14c4ca1', 'output': ''}\n",
            "{'id': 'fbe157c0-a0d2-4403-9aec-6d44057cebb6', 'output': '古赫胥氏之時，黎民百姓居處，皆為赫'}\n",
            "{'id': 'd15940f7-52d6-4f67-9f82-18d7a20af249', 'output': '翌明，錦衣衛陳鹵簿、儀仗於丹陛及丹墀，設明殿於錦衣衛，於錦衣衛內，於錦衣衛內，於錦衣衛內，於錦衣'}\n",
            "{'id': 'b74e5f5e-9521-41ff-a31a-f1d38d1f8b41', 'output': '因為殺人太多，鮮血流遍帳前，而楊素仍談笑自'}\n",
            "{'id': '2ff6719c-1b70-4656-8c8f-6775d83922ad', 'output': ''}\n",
            "{'id': '85ca3710-cf41-46b1-aac6-199f388cfae2', 'output': ''}\n",
            "{'id': 'db2cd06b-1fa8-48e9-97ac-fc0e77a25b95', 'output': '「各位朝臣們，我很快就要離開這個世界了，請�'}\n",
            "{'id': '7b05f621-1e96-4f1a-afb5-b52b1826e26e', 'output': ' '}\n",
            "{'id': '857a2290-2fad-4edd-9850-e24891c11b76', 'output': ' '}\n",
            "{'id': '5c735bff-d029-443e-9a31-c5c672829733', 'output': ' '}\n",
            "{'id': 'e647e01c-6be0-4201-b88a-e75e4bfab9ea', 'output': '\\n\\n將下麵句子翻譯成'}\n",
            "{'id': '5cc1255b-efc1-4e96-9824-938b9c37488d', 'output': '庫仁聽說公孫希打敗平�'}\n",
            "{'id': '91650fd4-6b91-474a-b301-9e0187d4f9a3', 'output': ''}\n",
            "{'id': 'ec0ac35d-af69-4c47-9152-e3ea8a60b3d8', 'output': '不順父母齣者，無子者，不順父母齣者，有子者，不順父母齣者，無子者，不順父母齣者，有子者，不順父母齣者'}\n",
            "{'id': '0bd01970-ca46-4029-ab5a-c31b68d550d6', 'output': ''}\n",
            "{'id': 'bf807777-1e4c-4c07-a24d-fb2e83e4642f', 'output': ''}\n",
            "{'id': '9d31ce9d-66bf-4179-b110-e644cd293ea3', 'output': ' '}\n",
            "{'id': 'aead802d-528b-4209-a9e0-580182a0fcdb', 'output': ''}\n",
            "{'id': '34f23d48-e2b3-4a02-929e-4066e4a612f7', 'output': ''}\n",
            "{'id': 'cae19ebb-3297-485f-bb43-b70013c7f0dd', 'output': '\\n久之相與寢處，窟中都自四虎，妻婦人者最老，妻婦人者最老，妻婦人者最老，妻婦人者最老，妻婦人者最老，妻婦人者最老，妻婦人者'}\n",
            "{'id': '9ac8a5af-b43a-40f4-8378-e3f973fc6819', 'output': '六部叫六政府，各部屬官叫從事，六科叫諫議，十三道叫直指使，各道屬官叫從事，各部屬官叫從事，各科屬官叫從事，各道屬官叫從事'}\n",
            "{'id': '2c0caaf9-904a-425f-b516-de4dd438d66c', 'output': '這句話的意思是說，彆人應該以自己的心為根據來要求'}\n",
            "{'id': '91da5e6d-733a-491f-ac3d-f089237c5211', 'output': ''}\n",
            "{'id': 'daaa6cad-b6f3-4110-93c4-c88fd3f74f1d', 'output': ''}\n",
            "{'id': '4248e35e-3518-43d8-aacb-233a3a606f60', 'output': ' '}\n",
            "{'id': '83085b49-c866-4a64-b91e-8e9d14577a6f', 'output': ''}\n",
            "{'id': '078a213f-4242-4145-8c3f-415728267aa8', 'output': ' '}\n",
            "{'id': '9d3cdcf9-8aab-4a7f-a2f9-4260677838db', 'output': '你'}\n",
            "{'id': '44fef78d-43d9-4b77-bc26-e90f1c34f042', 'output': '現在為你幫忙，這個人叫孔��'}\n",
            "{'id': '0590b9b9-5cda-4fdb-ab0a-e16b9a9ace36', 'output': ''}\n",
            "{'id': 'f4053bde-f37f-42de-83f1-45cb0605eaed', 'output': ' '}\n",
            "{'id': '53c5a76c-e8bc-4947-a112-063fd2804310', 'output': ' 殷士膚敏，裸將於京'}\n",
            "{'id': '78e69ebd-0930-4175-8657-b720e97d8cbb', 'output': ' 官吏受賄及倉庫官侵盜，颱察官知而不糾者，降一�'}\n",
            "{'id': '225aa4a9-42c7-4168-9904-a1e3706e5ef5', 'output': '又頒發冊書授予長孫無忌為司空，主持門下省、尚'}\n",
            "{'id': 'decd458c-5b25-40d5-8152-3049a0cdc79b', 'output': ''}\n",
            "{'id': 'd88714e8-6193-4d48-a455-035c64a9ce67', 'output': ''}\n",
            "{'id': '36e8de68-6958-4731-91c7-aaaba511310b', 'output': '「我聽說你很聰明，但我從未見�'}\n",
            "{'id': 'd0dc2cd8-9c6d-4653-a788-7a711f382cb6', 'output': ''}\n",
            "{'id': '308738e5-aa94-4605-b85c-fe75d55db907', 'output': ''}\n",
            "{'id': '9789b646-cc0d-47b4-9e8b-d1c4b7b44346', 'output': '士廉既任遇益隆，多所錶奏，成輒焚�'}\n",
            "{'id': '67078685-7438-4899-9568-ebb5526c4e75', 'output': ' '}\n",
            "{'id': '6857e25e-d735-4ae2-bead-7d6be5192ecc', 'output': ''}\n",
            "{'id': '4af423b9-cd08-4e87-8326-f6647857d2d2', 'output': ' '}\n",
            "{'id': '1c494ae3-1df0-4ce8-ae4f-3857864dec25', 'output': ' 元宵節燈宴，太後的母親��'}\n",
            "{'id': '52fc428d-d929-4b5a-8c68-0380808018b9', 'output': '「皇上，你'}\n",
            "{'id': '2fa8bbeb-ef2a-429a-9e60-dd82a8b9052e', 'output': '舊溝又東'}\n",
            "{'id': 'd4a22cb4-dff7-4f64-b2ae-0fec7d5e7eae', 'output': ' '}\n",
            "{'id': '258dbbe2-456b-4643-b301-eb7c73b5eaaf', 'output': ' '}\n",
            "{'id': '134dca67-ff20-4582-a6aa-7610a0c00822', 'output': ''}\n",
            "{'id': '42ae63a9-9eac-4e03-b068-4559915da042', 'output': ' 大臣歡悅啊！大臣歡悅啊！大臣歡悅啊！大臣歡悅啊！大臣歡悅啊！大臣歡'}\n",
            "{'id': '7ec8c29b-2271-4123-8bd4-201a8c1f203e', 'output': ' '}\n",
            "{'id': '5aee8323-eb45-42c4-a26e-71a8060a29b0', 'output': ' '}\n",
            "{'id': '27542736-3f3e-4724-9a5d-455110abc090', 'output': ' '}\n",
            "{'id': 'b83a164b-e810-4075-a220-2f4f70374232', 'output': '張元勛追擊敵賊，斬殺敵'}\n",
            "{'id': '03e35097-f6bf-4416-9974-8e8d5b622708', 'output': ' '}\n",
            "{'id': '0c8347db-407c-44e9-a91b-d476c58f0d51', 'output': ''}\n",
            "{'id': '7cfccb6e-ab89-4ae6-b38a-065ed249b12d', 'output': ' 那就把毛喜安置在一個小郡中，不許他再在朝�'}\n",
            "{'id': '7128f950-5ce6-48ad-9a15-e3a6df0a0ae9', 'output': ' '}\n",
            "{'id': '344253bc-8de4-47bc-bf05-11ec0bf6098f', 'output': '朕以為聚�'}\n",
            "{'id': '6297dca1-f3b3-4c85-b77b-5552dbcffe18', 'output': '吳喜於是攻破吳興，��'}\n",
            "{'id': '78100340-a292-4018-9162-0a2d57f1eb43', 'output': ' 占辭��'}\n",
            "{'id': '21d6a3cd-95bd-4573-8baf-91c25749ad72', 'output': ''}\n",
            "{'id': 'f99673b3-0492-475b-9d38-2ba82b089cfa', 'output': ' 王以上卿'}\n",
            "{'id': 'c90cba4e-1f75-4c16-9e6b-fe5f9fd3bc51', 'output': '唐劉希夷一名庭芝，汝州人，少有文華，喜��'}\n",
            "{'id': '46f0a062-4784-42f3-8671-d7d39d545521', 'output': '崇禎十七年五月，承恩提督京�'}\n",
            "{'id': '511e0eb3-5cba-4eee-955a-0c9dfa711513', 'output': '二月十五日，袁�'}\n",
            "{'id': 'fe517cf1-179e-4146-84f5-8704289883bf', 'output': '睏獸�'}\n",
            "{'id': 'b2c03c33-24d8-4643-abb1-26e1ab72cb19', 'output': ' '}\n",
            "{'id': '18bf435f-2702-4b46-aeb5-6ae18d4229c0', 'output': '\\n隨即煬帝又派使臣快遞詔書給諸葛亮，諸葛亮又派使臣快遞詔書給煬帝，煬帝又'}\n",
            "{'id': '842222cb-5de2-4e18-954d-4e7cc6f3f491', 'output': ' '}\n",
            "{'id': '43d19400-1474-4a17-a45f-c80305750a35', 'output': ' '}\n",
            "{'id': '6fd0aae8-c828-45fd-9b30-6554aeee7d17', 'output': ''}\n",
            "{'id': 'e0b4a07a-a132-4460-bf9d-30c12484fb36', 'output': ' '}\n",
            "{'id': '6eb517d1-62ac-42b0-b230-539ed2df366c', 'output': ' '}\n",
            "{'id': '7bc3756b-4133-4651-9f72-c5e122deba7f', 'output': ' '}\n",
            "{'id': '1c2a8368-9dc3-4fa8-9dac-fcfcfeb78e41', 'output': ' '}\n",
            "{'id': '049390ee-53ad-43d6-a8d7-b50a55a1cae8', 'output': ''}\n",
            "{'id': '0f796b1c-bb07-4319-8ecf-7798a0085ddf', 'output': ' '}\n",
            "{'id': '4ebe1471-f2df-449e-b178-f829c181ba59', 'output': ''}\n",
            "{'id': 'eb547ec2-f95f-4d86-9eb9-cc5afaab2260', 'output': '糧食是國家的基礎，但殿下無故散糧，虛損國庫，將'}\n",
            "{'id': '5aa2007b-cc41-4439-ac89-369b5b2b6cf4', 'output': '\\n賈夫的所生的兒子是劉彭祖、劉勝祖、劉彭祖、劉勝祖、劉彭祖、劉勝祖、劉彭祖、劉勝祖、劉彭祖、劉勝祖、劉彭'}\n",
            "{'id': 'd92ed4c7-5249-4b78-852f-c89c4a72bb1e', 'output': ''}\n",
            "{'id': 'f381f418-c157-4593-a581-274a973a1547', 'output': '\\n\\n將下麵句子�'}\n",
            "{'id': 'c15e3a8a-65f0-40f8-91ad-6ba048b0738f', 'output': ''}\n",
            "{'id': '9b5fe7a3-cc52-4754-a602-8cf167197284', 'output': '\\n\\n將下麵句子翻'}\n",
            "{'id': '0dd50bc7-3101-4236-842e-9ee387e35dcb', 'output': ' '}\n",
            "{'id': '5d77c673-3de1-4331-bade-debac1836760', 'output': ''}\n",
            "{'id': 'f3e0ae5d-f7fe-45f2-89f4-0dcb9a70b633', 'output': ' '}\n",
            "{'id': 'c15c3a01-d590-4851-a332-dfc29168f84c', 'output': ''}\n",
            "{'id': 'a91ef32e-6214-45ed-bb9c-db89e18b077f', 'output': ' '}\n",
            "{'id': '197daaf3-45b5-4881-ad63-17c70de742b2', 'output': '輪牙呈圓形，兩側各有十二個��'}\n",
            "{'id': '4a09f289-c7ec-47e6-9cc8-4ea68a9e9d78', 'output': ' 懸爵於朝，而有功者，封爵於朝，而有功者，封爵於朝，而有功者，封爵於朝，而有功者，封爵於朝，而有功者'}\n",
            "{'id': '67b3bc0e-7b79-444a-a94b-4043fcd037c6', 'output': '蘊自以國姻，不欲在內，苦求外齣；復以為都督浙'}\n",
            "{'id': '24845401-68ea-4436-9986-10d1aa3b715a', 'output': ' 姨酬其價，則�'}\n",
            "{'id': '705a6d3c-5111-4d49-801d-11ca332b4f59', 'output': ' '}\n",
            "{'id': 'bce16bfe-f45e-4982-b2cd-b9c62e160d30', 'output': ' '}\n",
            "{'id': '2288eb8c-cd16-45df-adc5-0e4a0ad29f6d', 'output': ' '}\n",
            "{'id': 'dbe6965a-e09c-45c2-9720-557240f1398c', 'output': '自從近代以來，親生母親能夠因兒子而被尊崇的，衹有兒子是藩王，或者兒子是親王，或者兒子是皇子，或者兒子是皇太子，或者兒子是皇太孫，或者兒子是皇孫，或者兒子'}\n",
            "{'id': '5ec0f8d4-e498-4c61-ad3a-e25af3831114', 'output': ' '}\n",
            "{'id': '94d97253-fdd9-4e72-a6b4-04ab1fdfe2c5', 'output': ' '}\n",
            "{'id': '805a71cb-dc94-4291-b9ed-a5138e2a1788', 'output': '\\n纍懼而遷於魯縣，立堯祠於西山，'}\n",
            "{'id': '91f6adef-40cd-438c-87aa-a1c5c5e03333', 'output': ' '}\n",
            "{'id': '7bca1449-5f3f-4746-95c2-a7caaf09183b', 'output': '「荊'}\n",
            "{'id': '746bbd9f-ae71-4086-ad16-8cef6f60bda2', 'output': ' 公姑'}\n",
            "{'id': '2fa8c2bb-db1d-4e13-b442-2430989f5209', 'output': '十二月，武皇在幽、鎮、定三州徵兵，準備到華州��'}\n",
            "{'id': '98641b88-477b-4a48-8c52-48c1f3e7b9d0', 'output': ''}\n",
            "{'id': '59b71970-0994-4a11-a55b-a02e12afb4e4', 'output': ''}\n",
            "{'id': 'd6f92ac9-ba36-4ebf-af12-77d129acf7ff', 'output': '「吾'}\n",
            "{'id': 'e9247ad2-c5f5-4688-9ce4-9fd7b1b55013', 'output': ''}\n",
            "{'id': '85610099-5a7e-4436-98ca-2458c8b15462', 'output': ' '}\n",
            "{'id': '1e002768-3c69-4762-9e19-c71cbb0fd1ef', 'output': ''}\n",
            "{'id': '8a5eec7e-a1a0-442f-bbac-6f3c4975e552', 'output': '七月初八夜，鳳翔李茂貞發兵攻擊劉景宣，劉景宣逃跑，鳳翔李茂貞追趕，劉景'}\n",
            "{'id': '0fedfeb5-acf7-4f45-ae01-805e2e15e35b', 'output': ' 你'}\n",
            "{'id': '17e5eb78-50be-4654-806c-736f5f0b0c32', 'output': '你'}\n",
            "{'id': '56e8217a-6f2e-46ef-aa13-2982012f28f5', 'output': ' '}\n",
            "{'id': '4321b379-4137-4f31-a3a3-9b2935bf6606', 'output': ' '}\n",
            "{'id': 'a245da15-a75c-4369-bb82-8d9464d2d6f0', 'output': ''}\n",
            "{'id': '33280e94-3da5-4588-a016-db84b7cf4667', 'output': ' '}\n",
            "{'id': 'a0405cab-d939-4ad1-9388-982a3dd98649', 'output': ''}\n",
            "{'id': 'f24a6494-1a7e-40aa-868a-9009fc3ae46d', 'output': ' 大祭祀，主管祭器的官員鬱人和掌管行政區劃的官�'}\n",
            "{'id': '4462a4ff-2d4b-4bf1-a1a6-99a43df440d8', 'output': ''}\n",
            "{'id': '9daaa0f6-323e-4fec-ad18-3f2af6ce4015', 'output': ' '}\n",
            "{'id': '32c8a4b4-0d54-4ed9-af35-d2e3e0d7931b', 'output': '諸王即將啓程趕赴封國，印璽綬帶何其鼎盛麗紅，印璽綬帶何其鼎盛麗紅，印璽綬帶何其鼎盛麗紅，印�'}\n",
            "{'id': '5c469282-d261-43ec-a081-a90ade4604c2', 'output': ' '}\n",
            "{'id': 'fe191f74-f0ae-43d4-9c4f-7300b4851e53', 'output': '朕對於這種狀況感到懊悔，'}\n",
            "{'id': '37364f2a-aa95-44a0-b18a-5b786e26adda', 'output': '五月庚辰日，武帝前往正武殿，�'}\n",
            "{'id': '86d50638-7a3d-41ca-8617-f2d13456bcc5', 'output': '超既西，先至於闐，廣德禮意甚疏，且其俗信巫，巫者甚為禮，故廣德禮意甚疏，且其俗信巫，巫者甚為禮，故廣德禮意�'}\n",
            "{'id': '8962f18b-c735-4407-85ef-9ab6d9f7fe39', 'output': '「柴又玄，柴又玄，柴又玄，柴又玄，柴又玄，柴又��'}\n",
            "{'id': '2d773827-80fa-4766-9131-e2001155e90f', 'output': ' 陸贄認為， 賢明的君主選擇將領，委'}\n",
            "{'id': 'f4a91058-3837-4c26-9d48-2209ac1029a2', 'output': ' 吾欲取��'}\n",
            "{'id': 'ec6fcc8a-3069-46ab-a15a-df9beb02491d', 'output': '\\n三月，檜孫敷文閣待製塤試進士舉，檜孫敷與檜孫敷文閣待製塤試進士舉，檜孫敷與檜孫敷'}\n",
            "{'id': '8d045d70-4bce-4469-8946-c6acbfa0d860', 'output': ' 這是楊光遠對劉處讓錶示�'}\n",
            "{'id': '83e2e125-02c6-4174-8fb6-17f626019c85', 'output': ' '}\n",
            "{'id': 'c30e6494-d570-4555-89b2-111c0f1c57f9', 'output': ' '}\n",
            "{'id': 'dd5ca327-6848-4c9b-aa35-a2ff3083df83', 'output': ''}\n",
            "{'id': '65cd8445-724b-4117-9b6a-f5ec021b9cbc', 'output': '嗣佩將印，勁兵重鎮，皆��'}\n",
            "{'id': 'c7a3b8ad-21d4-40e4-b52c-a690a6ffd43e', 'output': ''}\n",
            "{'id': '005c04ae-b012-4770-8af6-b6bce5f0a15f', 'output': ' 翕侯趙信為諸將之首，諸��'}\n",
            "{'id': '041e5ef3-d12b-4967-805b-da46b97ff8aa', 'output': '漢明帝時，水池中養有分枝荷，一莖生四葉，樣子就像四'}\n",
            "{'id': 'c1938f2a-5c09-48f3-94ae-40e619d4f2df', 'output': '\\n\\n翻譯'}\n",
            "{'id': '80492aa5-c3d0-4c6c-9bf1-6432b5c1c584', 'output': '若據伊尹'}\n",
            "{'id': '35d93da9-7f4e-40ff-8310-6e647b40da55', 'output': '李繼能鞭打母親管收藏的婢女，追索金銀，因而鞭打緻死，那婢女的傷'}\n",
            "{'id': '6613ac0a-92db-4ab8-861b-d092ded57320', 'output': ' '}\n",
            "{'id': 'c1904a85-43c0-4806-ac2c-dd79a5494c69', 'output': ' '}\n",
            "{'id': '82da739e-4ece-49ff-a2e8-e6bf9f37c42a', 'output': '李諤認為禮教凋敝，公卿剛死，其愛妾、侍婢，就被'}\n",
            "{'id': '037d5ecc-03ce-4748-88e0-ca3d84396263', 'output': '\\n於是作 瀋命法 ，曰群盜起不發覺，發覺則殺'}\n",
            "{'id': '58d2fddc-d4c4-4dfc-8e7b-3373d11e8f4b', 'output': ' '}\n",
            "{'id': '82ca008e-1f92-4978-bb50-ef7ba4c23406', 'output': ' 問'}\n",
            "{'id': '405b0fab-ac00-4fe5-a70d-1b85cb1eb5c4', 'output': '右丞相平徙為左丞相，太尉勃為右丞相，大將軍灌嬰為左丞相，左將軍鄧艾為右丞相，尚書令孔融為左丞相，尚書令孔和為右丞'}\n",
            "{'id': 'c795e8da-6602-441b-9632-a6eb4d5e429f', 'output': ''}\n",
            "{'id': '783cf9ec-8680-4df2-9cc6-009a1a53e241', 'output': '如不閤高皇帝、孝惠皇帝、孝文皇帝、孝武皇帝、孝昭皇帝、孝宣皇帝、太上皇、孝文太後、孝武太后、孝宣太后、太上太后、太上太后、太上太后、太上太后、太上太后、太上太后、太上太后、太上太后、太上太后、太上太后、太'}\n",
            "{'id': 'da56ff41-d28d-424a-92df-74510036ac3a', 'output': '黃龍三年春二月，孫權派遣太常潘濬率領五萬部眾徵討袁紹，袁紹派遣將軍袁譚率領五萬部眾對抗，孫權派遣'}\n",
            "{'id': '2dfd7834-e59f-4f10-a865-0bdbdbb48ed0', 'output': ' '}\n",
            "{'id': '440236ed-2454-4c32-9e4b-21a4353b5368', 'output': ' 我們兩個都是叛賊，怎麼可能討伐我呢？ 劉'}\n",
            "{'id': 'aefb9ce0-12ac-49c1-a5ce-c83166b41c18', 'output': ''}\n",
            "{'id': 'de4f7bce-fdbc-47e4-aeb5-151c08b5905b', 'output': '元嵩派遣兼統軍李叔仁等人援助閤肥、小峴、楊石，接著元嵩派遣兼統軍李叔仁等人援助閤肥、小峴、楊石，接著元嵩派遣��'}\n",
            "{'id': '0a1fbcf9-389e-44e2-9152-74930293659c', 'output': ' '}\n",
            "{'id': 'd029d6f7-63f4-4109-829d-09bf3a696040', 'output': ' '}\n",
            "{'id': 'b6b8dca0-aed4-479a-b5b2-190786ad9210', 'output': '九疇節儉而沒有彆的愛好，公事完後便焚香讀書，廉��'}\n",
            "{'id': 'bb678611-565b-4be7-a2a2-a392da3c13f8', 'output': '奔喪 '}\n",
            "{'id': '64696fc0-a6c0-4f3f-b5b8-378887952c81', 'output': ' '}\n",
            "{'id': '9351f26a-f9ae-45ff-96f1-215d2ed0b552', 'output': ' '}\n",
            "{'id': '553ca84e-2e1e-4481-8b9b-327eead6063f', 'output': '\\n三奏金革，四門齣兵，連旗萬計，風馳雲走，雷震雷震，雷震雷震，雷震雷震，雷震雷震，雷震雷震'}\n",
            "{'id': '2a6a376b-eb34-4041-b407-35c4c8c130d6', 'output': ' 貧賤傢兒忽得富貴，為何不感恩？\\n\\n\\n將下麵��'}\n",
            "{'id': '83b032da-4a94-4693-ba28-98bdfa3b3da7', 'output': '叛軍大將丁文豪，在皂莢橋擊敗官軍，一直挺進到硃雀桃城，叛軍將領楊廷麟、楊廷麟、楊廷麟、楊廷麟、楊廷麟、�'}\n",
            "{'id': '9ec30e83-67f8-4eb3-95d1-c73d291c3bd2', 'output': ''}\n",
            "{'id': '5d4c6c3f-7b42-4eda-b193-d2768353088b', 'output': ' '}\n",
            "{'id': '55e061df-ae50-4738-a417-dd21901d2c16', 'output': '這也是因為夏民貪婪、忿戾的風氣一天天盛行，殘害瞭夏'}\n",
            "{'id': '523e4b59-2455-4343-bd1b-2d9878763e6f', 'output': ' 你'}\n",
            "{'id': '83cf7c79-084d-446e-947a-03f4b485a8ce', 'output': '瞭大理王的兒子瞭瞭瞭'}\n",
            "{'id': 'dee25add-cfd0-48d1-8518-cbea8c0c8616', 'output': ''}\n",
            "{'id': '0156f07a-f041-4bec-9f04-2bc4aa6944d6', 'output': '\\n古史倉頡覽二象之爻，觀鳥獸之跡，彆彆彆彆彆彆彆彆彆彆彆彆彆彆彆彆彆彆彆彆彆彆��'}\n",
            "{'id': '1b3bdd15-8b7e-44f6-af30-d57e9cbb247d', 'output': ' '}\n",
            "{'id': 'cf09be06-cf32-4502-901d-821afb8e03c1', 'output': '十八章，齊閔王失國，問之，固因齊閔王殺'}\n",
            "{'id': '364fee8a-45bc-4f58-92d9-cafc8085fe09', 'output': '你'}\n",
            "{'id': 'bdb5da27-91b9-428d-ba77-921a53e174f4', 'output': ''}\n",
            "{'id': '4cce3ee9-5028-45da-b0fa-c9703c4216e0', 'output': ''}\n",
            "{'id': 'd4f9355b-bbfc-4d64-858b-fdb78487a831', 'output': ''}\n",
            "{'id': '58ad0068-f7a0-45d1-9eda-2235f0fef5f2', 'output': ' '}\n",
            "{'id': 'e860237d-16f7-499a-897f-21fddf10c71a', 'output': ''}\n",
            "{'id': '654a9636-a516-4418-87b4-4588be032615', 'output': ' '}\n",
            "{'id': '184b7041-65d4-469e-af51-b029ebc2ac69', 'output': ''}\n",
            "{'id': 'ef04862d-5298-4b07-aacf-5f815343db16', 'output': ''}\n",
            "{'id': '2ee814fb-fc56-42fd-8ca9-f146c2f01619', 'output': '初，公祏反，矯伏威令以紿眾，趙郡王孝恭既平公祏，得反叛眾，又與趙郡王孝恭為兄弟，又與趙郡王孝恭為兄弟，又與�'}\n",
            "{'id': 'a7d9767d-843e-4eca-8541-25765a7fc475', 'output': ' '}\n",
            "{'id': 'd3eb5c53-4373-4d33-af5c-0d0b624c7769', 'output': '莊襄王封相國呂不韋為文信侯，將河南洛陽為其領'}\n",
            "{'id': 'f3db6078-fe6b-46ac-b9e2-bf5f1be8f124', 'output': '\\n壬寅，詔以軍民不相統'}\n",
            "{'id': '95954dbd-ed2a-460f-86b5-8e8827a9d69a', 'output': '早晨與太陽閤，觀察不到，十九天'}\n",
            "{'id': '1f9eff85-4fb6-4d9a-a553-53ce81e1b4a3', 'output': ''}\n",
            "{'id': '0f71af04-a840-4bd1-aa7f-d113f0082cbb', 'output': ' '}\n",
            "{'id': '81380878-a112-46e0-b95c-1c3403766790', 'output': '\\n\\n翻�'}\n",
            "{'id': '324670da-f568-405f-becc-efcc0f38428d', 'output': '\\n\\n問��'}\n",
            "{'id': '0b9dd79b-c057-4b1c-8576-5779194f14f1', 'output': '\\n又嘗訪群臣可大任者，杞薦張鎰、嚴郢、徐庶、徐庶之弟徐庶之弟徐庶之弟徐庶之弟徐庶之弟徐庶'}\n",
            "{'id': 'b70302d5-adbd-4d99-87dd-c41a22c6fda3', 'output': '大學士楊一清、張璁等人多次上疏請求慶賀，禦史鄞人周相抗疏反對，認為慶賀將導致禮儀敗壞，並認為慶賀將導��'}\n",
            "{'id': 'da7aa2d3-e476-44f0-b403-f26b59479676', 'output': '不過幾日，白馬撒尿，濺在白鱉上，白�'}\n",
            "{'id': '96a49b49-d865-46e7-a798-d137e5a3e417', 'output': '久之，世祖又命護軍將軍尹悅、平東將軍杜幼安、巴州刺史杜幼實、徐州刺史杜幼實、徐州刺史杜幼實、徐州刺史杜幼實、'}\n",
            "{'id': 'e784e887-133e-424a-9d37-944a887d9314', 'output': '由南香東北大廠逾山，則高壑重疊，路小而近；由南香東南陽橋礦逾東廠，則高壑重疊，路小而近；由南香西北大廠逾山，則高壑重疊，路小而近；由南香西南陽橋礦逾'}\n",
            "{'id': '6453efe2-0044-4d1f-b449-940825acd052', 'output': ' '}\n",
            "{'id': '86a5b90e-f3b7-4649-b9ff-61df1deac684', 'output': ''}\n",
            "{'id': 'a492d22d-fa2d-4aec-b3c1-4be857a552d9', 'output': ''}\n",
            "{'id': 'cbd01967-035d-4ff6-8332-6048cd74323b', 'output': ''}\n",
            "{'id': 'adf3003c-c22b-4498-8ac5-1c4f4608427c', 'output': ' '}\n",
            "{'id': '08b7d037-dde0-47be-a4c5-fa07a649ebea', 'output': '朔方先鋒兵馬使白元光閤迴紇兵於靈颱，會雪雰嚴晦，攻'}\n",
            "{'id': '096a1c8c-371b-401e-9b8d-1f184299cccd', 'output': '是年，琉球、日本、安南、占城、真臘、爪'}\n",
            "{'id': 'f7637f39-91c8-4f34-bf38-d3be889eb198', 'output': ''}\n",
            "{'id': 'e881aba2-4179-45e7-ba3b-f545282aa9b1', 'output': ''}\n",
            "{'id': 'ef0443e9-9f57-41df-a95a-ec2eaf290c23', 'output': '父有是'}\n",
            "{'id': 'd4711a31-2da2-4d50-b033-ac29198c2745', 'output': '但是，李彪自以為得到'}\n",
            "{'id': '758fe868-3091-4558-82cc-d690bc336bef', 'output': ''}\n",
            "{'id': '698dec27-f850-4756-83c1-08f37ac71fa7', 'output': ' '}\n",
            "{'id': '38dd708e-1545-41f3-9620-5ac42fc7eee9', 'output': ' 吾所為，賈人輒知，益居'}\n",
            "{'id': '72ac8fc9-f152-47a7-80e0-b15968f9f74c', 'output': '在��'}\n",
            "{'id': 'dc879a8c-2fab-4ad4-891f-1af1c666ffa5', 'output': '九年，'}\n",
            "{'id': '7d2d2d3e-4989-4403-99a6-8ed86f9e1eff', 'output': ' '}\n",
            "{'id': '4ecbf031-bb40-4bc0-b43a-39d193ef370d', 'output': '司馬相如又寫一賦獻給漢武帝，漢武帝又大大感�'}\n",
            "{'id': '7eeb751d-873d-4164-8033-5cb91161280d', 'output': '暗地裏他又指使他的兒子曩烏用毒酒毒��'}\n",
            "{'id': 'da20ba9a-b967-4319-a9f4-5eeb2d4273e1', 'output': ' '}\n",
            "{'id': 'aa325434-eb42-438e-8eb6-6e030e5197a5', 'output': '所以應當對他錶示特殊的禮�'}\n",
            "{'id': '0c91c2f9-2851-409f-8db6-a064ba089b54', 'output': '潘捷餘因為倪按院姓��'}\n",
            "{'id': '4cd3e015-8733-4b97-8ce5-69fe8fafee84', 'output': '君主有主見地采取一種中肯意見，就不會有掉入臣下所設的陷'}\n",
            "{'id': '94e39235-1a50-4fb0-a4cb-6dafbb286afc', 'output': ''}\n",
            "{'id': 'a85afb95-0770-4360-9305-b1239a3530e8', 'output': ' '}\n",
            "{'id': 'bb42aa8d-1350-4349-85df-94a1234b9174', 'output': ''}\n",
            "{'id': '501f22a0-5e05-47e8-9c26-e8e2bf84acc4', 'output': '何況我為國效勞的願望已成泡影，而歸居傢園的願望'}\n",
            "{'id': '97afe65c-d407-40b4-ae82-2f9abd4e50af', 'output': ''}\n",
            "{'id': '34b29426-c87f-46b1-93d2-dd3cfdc1a7b8', 'output': ' '}\n",
            "{'id': '5539eb75-c9ad-4e50-b3b8-b4db8a89563b', 'output': ''}\n",
            "{'id': '4845e684-05a7-4eaa-9744-8cae8e1b5110', 'output': '而且諸侯國解散'}\n",
            "{'id': '6061f377-310b-474d-9f2c-97e7a81a6c9e', 'output': '足太陽之下，血氣盛則跟肉滿，踵堅；氣少血多則跟肉薄，踵軟；氣少血少則跟肉少，踵弱；氣多血少則跟肉多，��'}\n",
            "{'id': '68db9695-f952-4df0-aa92-05498e769ba7', 'output': ''}\n",
            "{'id': 'e35efd34-98ae-446f-8f28-77333a7d902f', 'output': ''}\n",
            "{'id': 'd931cdbf-105f-4bb2-b0a7-a03ecaf35fd0', 'output': ''}\n",
            "{'id': 'bd84867f-4f46-49df-ab96-4a707849d7fb', 'output': '「宜陽危急，韓君派使者到楚催兵求援，使者去瞭一批將�'}\n",
            "{'id': 'e5057776-2691-4502-8638-b306280c4c5b', 'output': ''}\n",
            "{'id': 'afa983e4-1a19-4ac3-a846-2090901b00e4', 'output': ' '}\n",
            "{'id': '0b5fbd4f-a313-4568-84b1-0f1ef4a525f2', 'output': ''}\n"
          ]
        }
      ],
      "source": [
        "for x in test_result_lst:\n",
        "    if \"ASSISTANT\" in x[\"output\"]:\n",
        "        x[\"output\"] = x[\"output\"].split(\"ASSISTANT: \")[-1]\n",
        "    if \"USER\" in x[\"output\"]:\n",
        "        x[\"output\"] = x[\"output\"].split(\"USER: \")[-1]\n",
        "    if \"答案\" in x[\"output\"]:\n",
        "        x[\"output\"] = x[\"output\"].split(\"答案：\")[-1]\n",
        "    if \"：\" in x[\"output\"]:\n",
        "        x[\"output\"] = x[\"output\"].split(\"：\")[-1]\n",
        "    if \"。\" in x[\"output\"]:\n",
        "        x[\"output\"] = x[\"output\"].split(\"。\")[-1]\n",
        "    if \"你是\" in x[\"output\"]:\n",
        "        x[\"output\"] = x[\"output\"].split(\"你是\")[0]\n",
        "    if \"你要\" in x[\"output\"]:\n",
        "        x[\"output\"] = x[\"output\"].split(\"你要\")[0]\n",
        "\n",
        "for x in test_result_lst:\n",
        "    print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZqhSS9Mn6kZ-"
      },
      "outputs": [],
      "source": [
        "with open(args.result_file, \"w\") as f:\n",
        "    json.dump(test_result_lst, f, indent=4)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c93c7cf6a2b479d9b9f9e9c6bca24ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37fe5ca34ce94eadaa13307613bef13a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43fdac7e6e814fb9bb70d3a83e14d70e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4db492a34dd340febaab49847e0a5011": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a74a0636613462e86a7a4ce27af594a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a3f66bf9bf04efea1ecdd99b029da2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a74a0636613462e86a7a4ce27af594a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cda2b0e3d3534f22a71d02bc0485187a",
            "value": 2
          }
        },
        "7bdeee7048a44c89a7887714350b6eef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a167cc80bdad4c9dbb629e7cd053f203": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bdeee7048a44c89a7887714350b6eef",
            "placeholder": "​",
            "style": "IPY_MODEL_1c93c7cf6a2b479d9b9f9e9c6bca24ee",
            "value": " 2/2 [00:20&lt;00:00,  9.35s/it]"
          }
        },
        "be3f6d2f3a7f44109df8ff3f9d2b5b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4db492a34dd340febaab49847e0a5011",
            "placeholder": "​",
            "style": "IPY_MODEL_37fe5ca34ce94eadaa13307613bef13a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cda2b0e3d3534f22a71d02bc0485187a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb77f7e7198848c9bec36321f093f282": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be3f6d2f3a7f44109df8ff3f9d2b5b3b",
              "IPY_MODEL_7a3f66bf9bf04efea1ecdd99b029da2c",
              "IPY_MODEL_a167cc80bdad4c9dbb629e7cd053f203"
            ],
            "layout": "IPY_MODEL_43fdac7e6e814fb9bb70d3a83e14d70e"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
